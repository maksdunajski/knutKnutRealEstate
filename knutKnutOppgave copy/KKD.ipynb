{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6452f6b192c7ec1",
   "metadata": {},
   "source": [
    "# KKD Real Estate Price Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e8da7",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f21c22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.419237Z",
     "start_time": "2025-11-21T10:55:05.414238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\oleho\\\\ikt110h25\\\\handin2\\\\handin2\\\\knutKnutOppgave'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63354ee073bb56dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.490731Z",
     "start_time": "2025-11-21T10:55:05.455747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load data function\n",
    "def load_data(path: str):\n",
    "    path = Path(path)\n",
    "    with path.open('r') as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Load datasets\n",
    "houses = load_data(\"data/houses_clean.jsonl\")\n",
    "agents = load_data(\"data/agents.jsonl\")\n",
    "districts = load_data(\"data/districts.jsonl\")\n",
    "schools = load_data(\"data/schools.jsonl\")\n",
    "\n",
    "# Convert to lookup dicts\n",
    "agent_map = {a[\"agent_id\"]: a for a in agents}\n",
    "district_map_full = {d[\"id\"]: d for d in districts}\n",
    "school_map_full = {s[\"id\"]: s for s in schools}\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_encode_int_matrix(int_matrix, n_classes_per_feature):\n",
    "    \"\"\"\n",
    "    int_matrix: shape (n_samples, n_cat_features)\n",
    "    n_classes_per_feature: list with number of classes for each categorical feature\n",
    "    Returns:\n",
    "        one_hot: np.array shape (n_samples, sum(n_classes_per_feature))\n",
    "    \"\"\"\n",
    "    n_samples, n_features = int_matrix.shape\n",
    "    oh_columns = []\n",
    "\n",
    "    for j in range(n_features):\n",
    "        col = int_matrix[:, j].astype(int)\n",
    "        n_classes = n_classes_per_feature[j]\n",
    "\n",
    "        # Allocate zeros\n",
    "        oh = np.zeros((n_samples, n_classes), float)\n",
    "\n",
    "        # Mask valid categories (>=0)\n",
    "        mask = col >= 0\n",
    "        oh[np.where(mask)[0], col[mask]] = 1.0\n",
    "\n",
    "        oh_columns.append(oh)\n",
    "\n",
    "    return np.hstack(oh_columns) if oh_columns else np.empty((n_samples, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b0cfd",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Add districts and school features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4496e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.505749Z",
     "start_time": "2025-11-21T10:55:05.497739Z"
    }
   },
   "outputs": [],
   "source": [
    "for h in houses:\n",
    "    # District\n",
    "    d = district_map_full.get(h[\"district_id\"], {})\n",
    "    h[\"district_crime\"] = d.get(\"crime_rating\", 0)\n",
    "    h[\"district_transport\"] = d.get(\"public_transport_rating\", 0)\n",
    "\n",
    "    # School\n",
    "    s = school_map_full.get(h[\"school_id\"], {})\n",
    "    h[\"school_rating\"] = s.get(\"rating\", 0)\n",
    "    h[\"school_capacity\"] = s.get(\"capacity\", 0)\n",
    "    h[\"school_age\"] = 2025 - s.get(\"built_year\", 2025)\n",
    "\n",
    "    # House age\n",
    "    year_built = h.get(\"year\", 2025)\n",
    "    remodel = h.get(\"remodeled\", -1)\n",
    "    if remodel > 0:\n",
    "        h[\"effective_age\"] = 2025 - remodel\n",
    "    else:\n",
    "        h[\"effective_age\"] = 2025 - year_built\n",
    "\n",
    "    # price per m2\n",
    "    size = h.get(\"size\", 0)\n",
    "    if size > 0:\n",
    "        h[\"price_per_m2\"] = h[\"price\"] / size\n",
    "    else:\n",
    "        h[\"price_per_m2\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177273e",
   "metadata": {},
   "source": [
    "## Define features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca6386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.523058Z",
     "start_time": "2025-11-21T10:55:05.514057Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Price prediciton features\n",
    "# =========================\n",
    "\n",
    "# # Numeric features\n",
    "# price_numeric_features = [\n",
    "#     \"condition_rating\", \"external_storage_m2\", \"kitchens\", \"lot_w\",\n",
    "#     \"size\", \"storage_rating\", \"sun_factor\",\n",
    "#     \"district_crime\", \"district_transport\",\n",
    "#     \"school_rating\", \"school_capacity\", \"school_age\",\n",
    "#     \"effective_age\"\n",
    "# ]\n",
    "\n",
    "price_numeric_features = ['size', 'external_storage_m2', 'lot_w']\n",
    "\n",
    "price_categorical_features = ['sold_in_month', 'agent_id']\n",
    "# Categorical features encoded as ints\n",
    "#price_categorical_features = [\n",
    "#    \"advertisement\", \"color\", \"fireplace\", \"parking\",\n",
    "#    \"rooms\"\n",
    "#]\n",
    "\n",
    "price_category_values = {\n",
    "    feature: sorted({h.get(feature, \"\") for h in houses})\n",
    "    for feature in price_categorical_features\n",
    "}\n",
    "\n",
    "price_category_maps = {\n",
    "    feature: {v: i for i, v in enumerate(vals)}\n",
    "    for feature, vals in price_category_values.items()\n",
    "}\n",
    "\n",
    "# ID encoding\n",
    "agent_id_map = {a[\"agent_id\"]: i for i, a in enumerate(agents)}\n",
    "district_id_map = {d[\"id\"]: i for i, d in enumerate(districts)}\n",
    "school_id_map = {s[\"id\"]: i for i, s in enumerate(schools)}\n",
    "\n",
    "# ========================\n",
    "# Days on marked features\n",
    "# ========================\n",
    "\n",
    "# Numeric features\n",
    "marked_numeric_features = [\n",
    "    \"condition_rating\", \"external_storage_m2\", \"kitchens\", \"lot_w\", \n",
    "    \"storage_rating\", \"sun_factor\",\n",
    "    \"district_crime\", \"district_transport\", \"school_capacity\",\n",
    "    \"price_per_m2\", \"size\"\n",
    "]\n",
    "\n",
    "# Categorical features encoded as ints\n",
    "marked_categorical_features = [\n",
    "    \"advertisement\", \"color\",\n",
    "    \"rooms\", \"agent_id\"\n",
    "]\n",
    "\n",
    "\n",
    "marked_category_values = {\n",
    "    feature: sorted({h.get(feature, \"\") for h in houses})\n",
    "    for feature in marked_categorical_features\n",
    "}\n",
    "\n",
    "marked_category_maps = {\n",
    "    feature: {v: i for i, v in enumerate(vals)}\n",
    "    for feature, vals in marked_category_values.items()\n",
    "}\n",
    "\n",
    "# ID encoding\n",
    "agent_id_map = {a[\"agent_id\"]: i for i, a in enumerate(agents)}\n",
    "district_id_map = {d[\"id\"]: i for i, d in enumerate(districts)}\n",
    "school_id_map = {s[\"id\"]: i for i, s in enumerate(schools)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507ead8",
   "metadata": {},
   "source": [
    "## Feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acfd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.568184Z",
     "start_time": "2025-11-21T10:55:05.543387Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Price prediction matrix (fixed)\n",
    "# =========================\n",
    "\n",
    "# ----- Numeric features -----\n",
    "price_num_data = np.array(\n",
    "    [[h.get(f, 0) for f in price_numeric_features] for h in houses],\n",
    "    float\n",
    ")\n",
    "\n",
    "# ----- Int-encoded categorical features -----\n",
    "price_cat_int = np.array([\n",
    "    [price_category_maps[f].get(h.get(f, \"\"), -1) for f in price_categorical_features]\n",
    "    for h in houses\n",
    "], int)\n",
    "\n",
    "# ----- One-hot encode categorical features -----\n",
    "price_n_classes = [len(price_category_maps[f]) for f in price_categorical_features]\n",
    "price_cat_oh = one_hot_encode_int_matrix(price_cat_int, price_n_classes)\n",
    "\n",
    "# ----- Combine numeric + one-hot -----\n",
    "X_price = np.hstack([price_num_data, price_cat_oh])\n",
    "y_price = np.array([[h[\"price\"]] for h in houses], float)\n",
    "\n",
    "# ----- Train/test split -----\n",
    "n = len(X_price)\n",
    "split = int(0.8 * n)\n",
    "\n",
    "X_train_price = X_price[:split]\n",
    "X_test_price  = X_price[split:]\n",
    "\n",
    "y_train_price = y_price[:split]\n",
    "y_test_price  = y_price[split:]\n",
    "\n",
    "# ----- Scaling -----\n",
    "# Only scale numeric features (first len(price_numeric_features) columns)\n",
    "X_mean = X_train_price[:, :len(price_numeric_features)].mean(axis=0)\n",
    "X_std  = X_train_price[:, :len(price_numeric_features)].std(axis=0) + 1e-8\n",
    "\n",
    "# Scale numeric columns; leave categorical one-hot unchanged\n",
    "X_train_price_scaled = X_train_price.copy()\n",
    "X_train_price_scaled[:, :len(price_numeric_features)] = (\n",
    "    X_train_price[:, :len(price_numeric_features)] - X_mean\n",
    ") / X_std\n",
    "\n",
    "X_test_price_scaled = X_test_price.copy()\n",
    "X_test_price_scaled[:, :len(price_numeric_features)] = (\n",
    "    X_test_price[:, :len(price_numeric_features)] - X_mean\n",
    ") / X_std\n",
    "\n",
    "# ----- Log-transform target only -----\n",
    "y_train_price_scaled = np.log1p(y_train_price)\n",
    "y_test_price_scaled  = np.log1p(y_test_price)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Days on marked features\n",
    "# ========================\n",
    "\n",
    "# ----- Numeric matrix -----\n",
    "marked_num_data = np.array(\n",
    "    [[h.get(f, 0) for f in marked_numeric_features] for h in houses],\n",
    "    float\n",
    ")\n",
    "\n",
    "# ----- Int-encoded categorical matrix -----\n",
    "marked_cat_int = np.array([\n",
    "    [marked_category_maps[f].get(h.get(f, \"\"), -1) for f in marked_categorical_features]\n",
    "    for h in houses\n",
    "], int)\n",
    "\n",
    "# Number of classes per categorical field\n",
    "marked_n_classes = [\n",
    "    len(marked_category_maps[f])\n",
    "    for f in marked_categorical_features\n",
    "]\n",
    "\n",
    "# ----- One-hot encode -----\n",
    "marked_cat_oh = one_hot_encode_int_matrix(marked_cat_int, marked_n_classes)\n",
    "\n",
    "# ----- Combine numeric + one-hot -----\n",
    "X_marked = np.hstack([marked_num_data, marked_cat_oh])\n",
    "y_marked = np.array([[h[\"days_on_marked\"]] for h in houses], float)\n",
    "\n",
    "# ----- Train/test split -----\n",
    "n = len(X_marked)\n",
    "split = int(0.8 * n)\n",
    "\n",
    "X_train_marked = X_marked[:split]\n",
    "X_test_marked  = X_marked[split:]\n",
    "\n",
    "y_train_marked = y_marked[:split]\n",
    "y_test_marked  = y_marked[split:]\n",
    "\n",
    "# ----- Scaling -----\n",
    "X_mean = X_train_marked.mean(axis=0)\n",
    "X_std  = X_train_marked.std(axis=0) + 1e-8\n",
    "\n",
    "X_train_marked_scaled = (X_train_marked - X_mean) / X_std\n",
    "X_test_marked_scaled  = (X_test_marked - X_mean) / X_std\n",
    "\n",
    "# y scaling (log transform only + no standardization!)\n",
    "y_train_marked_scaled = np.log1p(y_train_marked)\n",
    "y_test_marked_scaled  = np.log1p(y_test_marked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec86b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scalers to models\\scalers.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Price scaler (numeric cols only)\n",
    "price_num_len = len(price_numeric_features)\n",
    "X_mean_price = X_train_price[:, :price_num_len].mean(axis=0)\n",
    "X_std_price  = X_train_price[:, :price_num_len].std(axis=0) + 1e-8\n",
    "\n",
    "# Marked scaler (all columns were scaled)\n",
    "X_mean_marked = X_train_marked.mean(axis=0)\n",
    "X_std_marked  = X_train_marked.std(axis=0) + 1e-8\n",
    "\n",
    "scalers = {\n",
    "    \"price\": {\n",
    "        \"mean\": X_mean_price,\n",
    "        \"std\": X_std_price,\n",
    "        \"numeric_features\": price_numeric_features\n",
    "    },\n",
    "    \"marked\": {\n",
    "        \"mean\": X_mean_marked,\n",
    "        \"std\": X_std_marked,\n",
    "        \"numeric_features\": marked_numeric_features\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(models_dir / \"scalers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(f\"Saved scalers to {models_dir / 'scalers.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3dadd",
   "metadata": {},
   "source": [
    "## Simple dataset for testing SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7e7a59c665311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.580558Z",
     "start_time": "2025-11-21T10:55:05.575196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple linear dataset: y = 3x + 2 + noise\n",
    "np.random.seed(0)\n",
    "x_vals = np.linspace(-5, 5, 1000)  # more points\n",
    "noise = np.random.normal(scale=1.5, size=x_vals.shape)\n",
    "y_vals = 3 * x_vals + 2 + noise\n",
    "\n",
    "# reshape for your code:\n",
    "xs = x_vals.reshape(-1, 1)  # shape (m, 1)\n",
    "ys = y_vals.reshape(-1, 1)  # shape (m, 1)\n",
    "\n",
    "train_split = 0.8\n",
    "n_samples = int(xs.shape[0] * train_split)\n",
    "x_train = xs[:n_samples]\n",
    "y_train = ys[:n_samples]\n",
    "x_test = xs[n_samples:]\n",
    "y_test = ys[n_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04482c5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b61c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.593432Z",
     "start_time": "2025-11-21T10:55:05.587066Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(theta, xs, bias):\n",
    "    return np.dot(xs, theta) + bias\n",
    "\n",
    "def J_squared_residual(theta, bias, xs, y):\n",
    "    h = predict(theta, xs, bias)\n",
    "    sr = ((h - y) ** 2).sum()\n",
    "    return sr\n",
    "\n",
    "def gradient_J_squared_residual(theta, bias, xs, y):\n",
    "    \"\"\"Return gradient wrt theta (weights) and bias separately.\"\"\"\n",
    "    h = predict(theta, xs, bias)\n",
    "    # grad_theta shape: (n_features, 1)\n",
    "    grad_theta = np.dot(xs.transpose(), (h - y))\n",
    "    # grad_bias is scalar\n",
    "    grad_bias = (h - y).sum()\n",
    "    return grad_theta, grad_bias\n",
    "\n",
    "def mse(theta, bias, xs, y):\n",
    "    h = predict(theta, xs, bias)\n",
    "    return ((h - y) ** 2).mean()\n",
    "\n",
    "def train_model(xs, y, lr=0.01, batch_size=128, epochs=500):\n",
    "    # initialize parameters\n",
    "    n_features = xs.shape[1]\n",
    "    theta = np.zeros((n_features, 1))\n",
    "    bias = 0.0\n",
    "    m = xs.shape[0]\n",
    "    j_history = []\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle data at the start of each epoch\n",
    "        perm = np.random.permutation(m)\n",
    "        xs_shuffled = xs[perm]\n",
    "        y_shuffled = y[perm]\n",
    "        # process mini-batches\n",
    "        for i in range(0, m, batch_size):\n",
    "            x_batch = xs_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            b = x_batch.shape[0] \n",
    "            if b == 0:\n",
    "                continue\n",
    "\n",
    "            # compute gradients\n",
    "            grad_theta, grad_bias = gradient_J_squared_residual(theta, bias, x_batch, y_batch)\n",
    "            # theta -= lr * grad_theta * (1.0 / b) * batch_size\n",
    "            # bias -= lr * grad_bias * (1.0 / b) * batch_size\n",
    "            theta -= lr * (grad_theta / b)\n",
    "            bias  -= lr * (grad_bias / b)\n",
    "    \n",
    "        # compute and record loss at epoch end\n",
    "        j = J_squared_residual(theta, bias, xs, y)\n",
    "        j_history.append(j)\n",
    "        loss = mse(theta, bias, xs, y)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    return theta, bias, j_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60dfd3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2eccb0e54f4b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.699392Z",
     "start_time": "2025-11-21T10:55:05.603438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 174.33478104578919\n",
      "Epoch 100, Loss: 0.06638156289723694\n",
      "Epoch 200, Loss: 0.06427085054364365\n",
      "Epoch 300, Loss: 0.0638305966401651\n",
      "Epoch 400, Loss: 0.0637097532526886\n",
      "theta shape: (39, 1)\n",
      "bias: 13.887471423245778\n",
      "The L2 error is: 119.70\n",
      "The L1 error is: 349.76\n",
      "R^2: 0.94\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Train the price model\n",
    "# =====================\n",
    "theta_price, bias_price, j_history_price = train_model(\n",
    "    X_train_price_scaled, y_train_price_scaled)\n",
    "\n",
    "print(\"theta shape:\", theta_price.shape)\n",
    "print(\"bias:\", bias_price)\n",
    "\n",
    "# append the final result.\n",
    "j = J_squared_residual(theta_price, bias_price, X_train_price_scaled, y_train_price_scaled)\n",
    "j_history_price.append(j)\n",
    "print(\"The L2 error is: {:.2f}\".format(j))\n",
    "\n",
    "# find the L1 error.\n",
    "y_pred = predict(theta_price, X_train_price_scaled, bias_price)\n",
    "l1_error = np.abs(y_pred - y_train_price_scaled).sum()\n",
    "print(\"The L1 error is: {:.2f}\".format(l1_error))\n",
    "\n",
    "# Find the R^2\n",
    "u_price = ((y_train_price_scaled - y_pred) ** 2).sum()\n",
    "v_price = ((y_train_price_scaled - y_train_price_scaled.mean()) ** 2).sum()\n",
    "print(\"R^2: {:.2f}\".format(1 - (u_price / v_price)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee73e10c99821e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.867865Z",
     "start_time": "2025-11-21T10:55:05.736608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.191511926996053\n",
      "Epoch 100, Loss: 0.7992865687277668\n",
      "Epoch 200, Loss: 0.7964251567098368\n",
      "Epoch 300, Loss: 0.7959819139605903\n",
      "Epoch 400, Loss: 0.7959188287325456\n",
      "theta shape: (51, 1)\n",
      "bias: 2.398645657988257\n",
      "The L2 error is: 1496.35\n",
      "The L1 error is: 1442.83\n",
      "R^2: 0.17\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Train the marked model\n",
    "# =====================\n",
    "\n",
    "theta_marked, bias_marked, j_history_marked = train_model(\n",
    "    X_train_marked_scaled, y_train_marked_scaled)\n",
    "\n",
    "print(\"theta shape:\", theta_marked.shape)\n",
    "print(\"bias:\", bias_marked)\n",
    "\n",
    "# append the final result.\n",
    "j = J_squared_residual(theta_marked, bias_marked, X_train_marked_scaled, y_train_marked_scaled)\n",
    "j_history_marked.append(j)\n",
    "print(\"The L2 error is: {:.2f}\".format(j))\n",
    "\n",
    "# find the L1 error.\n",
    "y_pred = predict(theta_marked, X_train_marked_scaled, bias_marked)\n",
    "l1_error = np.abs(y_pred - y_train_marked_scaled).sum()\n",
    "print(\"The L1 error is: {:.2f}\".format(l1_error))\n",
    "\n",
    "# Find the R^2\n",
    "u_marked = ((y_train_marked_scaled - y_pred) ** 2).sum()\n",
    "v_marked = ((y_train_marked_scaled - y_train_marked_scaled.mean()) ** 2).sum()\n",
    "print(\"R^2: {:.2f}\".format(1 - (u_marked / v_marked)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772f8ae831f14ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.883940Z",
     "start_time": "2025-11-21T10:55:05.877938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.05952922718938116\n",
      "Test MAE: 0.17849082771471583\n",
      "Test R²: 0.9322930304754892\n",
      "y_hat: [2312387.04557469], y: [1654000.] , error [0.71527818]\n",
      "y_hat: [17326887.91398527], y: [17175228.99999997] , error [0.99124719]\n",
      "y_hat: [1098477.00874823], y: [1920489.] , error [1.74831971]\n",
      "y_hat: [12821574.83582105], y: [15386719.] , error [1.20006467]\n",
      "y_hat: [956991.59850959], y: [1691010.] , error [1.76700611]\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Evaluate price model\n",
    "# =======================\n",
    "\n",
    "# predictions on unseen test data\n",
    "y_pred_log = predict(theta_price, X_test_price_scaled, bias_price)\n",
    "\n",
    "# MSE\n",
    "test_mse = ((y_pred_log - y_test_price_scaled) ** 2).mean()\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "# MAE (L1)\n",
    "test_mae = np.abs(y_pred_log - y_test_price_scaled).mean()\n",
    "print(\"Test MAE:\", test_mae)\n",
    "\n",
    "# R^2\n",
    "u = ((y_test_price_scaled - y_pred_log) ** 2).sum()\n",
    "v = ((y_test_price_scaled - y_test_price_scaled.mean()) ** 2).sum()\n",
    "test_r2 = 1 - (u / v)\n",
    "print(\"Test R²:\", test_r2)\n",
    "\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y = np.expm1(y_test_price_scaled)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"y_hat: {y_pred[i]}, y: {y[i]} , error {y[i] / y_pred[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fba754fd31dd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.913274Z",
     "start_time": "2025-11-21T10:55:05.905841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DAYS] RMSE (normalized): 0.9138718400833414\n",
      "[DAYS] R^2 (normalized): 0.165\n",
      "[DAYS] RMSE (days): 18.63\n",
      "[DAYS] MAE (days): 12.53\n",
      "y_hat: 10.35, y: 4.40, error -5.95\n",
      "y_hat: 8.66, y: 4.00, error -4.66\n",
      "y_hat: 5.11, y: 4.00, error -1.11\n",
      "y_hat: 11.88, y: 4.90, error -6.98\n",
      "y_hat: 2.85, y: 2.10, error -0.75\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Evaluate marked model\n",
    "# =======================\n",
    "\n",
    "# predictions on unseen test data (log-scale)\n",
    "y_pred_log = predict(theta_marked, X_test_marked_scaled, bias_marked)\n",
    "\n",
    "# MSE and RMSE on log-scale\n",
    "mse_log = ((y_pred_log - y_test_marked_scaled) ** 2).mean()\n",
    "rmse_log = np.sqrt(mse_log)\n",
    "\n",
    "# normalized RMSE: divide by std of the log targets\n",
    "rmse_normalized = rmse_log / (y_test_marked_scaled.std() + 1e-8)\n",
    "print(f\"[DAYS] RMSE (normalized): {rmse_normalized}\")\n",
    "\n",
    "# R^2 on log-scale (same as before)\n",
    "u = ((y_test_marked_scaled - y_pred_log) ** 2).sum()\n",
    "v = ((y_test_marked_scaled - y_test_marked_scaled.mean()) ** 2).sum()\n",
    "test_r2 = 1 - (u / v)\n",
    "print(f\"[DAYS] R^2 (normalized): {test_r2:.3f}\")\n",
    "\n",
    "# Inverse transform to days\n",
    "y_pred_days = np.expm1(y_pred_log).ravel()\n",
    "y_true_days = np.expm1(y_test_marked_scaled).ravel()\n",
    "\n",
    "# RMSE and MAE in days\n",
    "rmse_days = np.sqrt(((y_pred_days - y_true_days) ** 2).mean())\n",
    "mae_days = np.abs(y_pred_days - y_true_days).mean()\n",
    "print(f\"[DAYS] RMSE (days): {rmse_days:.2f}\")\n",
    "print(f\"[DAYS] MAE (days): {mae_days:.2f}\")\n",
    "\n",
    "# Optional: show a few example predictions (days)\n",
    "for i in range(min(5, len(y_true_days))):\n",
    "    print(f\"y_hat: {y_pred_days[i]:.2f}, y: {y_true_days[i]:.2f}, error {y_true_days[i] - y_pred_days[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75accaf01c779f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:05.957434Z",
     "start_time": "2025-11-21T10:55:05.947917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('size',  0.48334179), ('external_storage_m2', -0.34670913),\n",
       "       ('lot_w',  0.21201762), ('sold_in_month_january',  1.18981163),\n",
       "       ('sold_in_month_february',  1.20434879),\n",
       "       ('sold_in_month_march',  1.03328772),\n",
       "       ('sold_in_month_april',  1.09166812),\n",
       "       ('sold_in_month_may',  1.03749455),\n",
       "       ('sold_in_month_june',  1.27071307),\n",
       "       ('sold_in_month_july',  1.33323048),\n",
       "       ('sold_in_month_august',  1.14292134),\n",
       "       ('sold_in_month_september',  1.2812051 ),\n",
       "       ('sold_in_month_october',  1.04048911),\n",
       "       ('sold_in_month_november',  1.12393895),\n",
       "       ('sold_in_month_december',  1.13836258),\n",
       "       ('agent_id_Alice Vadasy',  0.55906852),\n",
       "       ('agent_id_Rhonda Bowers',  0.59864523),\n",
       "       ('agent_id_Pedro Sasser',  0.61865167),\n",
       "       ('agent_id_Porfirio Wueste',  0.58783482),\n",
       "       ('agent_id_Donna Mcclintock',  0.56498134),\n",
       "       ('agent_id_Frank Schmidt',  0.57904482),\n",
       "       ('agent_id_John Rios',  0.56054699),\n",
       "       ('agent_id_Michael Hanners',  0.60671883),\n",
       "       ('agent_id_John Jiminez',  0.61758377),\n",
       "       ('agent_id_Frank Scheetz',  0.618162  ),\n",
       "       ('agent_id_Donald Campbell',  0.51762812),\n",
       "       ('agent_id_Kristen Webb',  0.61570332),\n",
       "       ('agent_id_Michael Rowland',  0.62798369),\n",
       "       ('agent_id_Charlotte Nodeine',  0.61269449),\n",
       "       ('agent_id_Ann Perez',  0.57616555),\n",
       "       ('agent_id_Shirly Delrio',  0.49588473),\n",
       "       ('agent_id_Lucy Moffatt',  0.57478933),\n",
       "       ('agent_id_Gonzalo Ramos',  0.54613499),\n",
       "       ('agent_id_Cheryl Dunlap',  0.55905244),\n",
       "       ('agent_id_Mary Chavez',  0.57710384),\n",
       "       ('agent_id_Pat Daniels',  0.54900035),\n",
       "       ('agent_id_Johnnie Stanley',  0.57749109),\n",
       "       ('agent_id_Misty Wallace',  0.61063901),\n",
       "       ('agent_id_Gregory Dixon',  0.53596249)],\n",
       "      dtype=[('feature', '<U50'), ('weight', '<f8')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_per_feature = [\n",
    "    ['january', 'february', 'march', 'april', 'may', 'june',\n",
    "     'july', 'august', 'september', 'october', 'november', 'december'], # sold_in_month\n",
    "    ['Alice Vadasy', 'Rhonda Bowers', 'Pedro Sasser', 'Porfirio Wueste', \n",
    "     'Donna Mcclintock', 'Frank Schmidt', 'John Rios', 'Michael Hanners', \n",
    "     'John Jiminez', 'Frank Scheetz', 'Donald Campbell', 'Kristen Webb', \n",
    "     'Michael Rowland', 'Charlotte Nodeine', 'Ann Perez', 'Shirly Delrio', \n",
    "     'Lucy Moffatt', 'Gonzalo Ramos', 'Cheryl Dunlap', 'Mary Chavez', 'Pat Daniels', \n",
    "     'Johnnie Stanley', 'Misty Wallace', 'Gregory Dixon'] # agent names\n",
    "  \n",
    "    ]\n",
    "\n",
    "n_classes_per_feature = [len(c) for c in categories_per_feature]\n",
    "one_hot_feature_names = []\n",
    "for feature_name, categories in zip(price_categorical_features, categories_per_feature):\n",
    "    for cat in categories:\n",
    "        one_hot_feature_names.append(f\"{feature_name}_{cat}\")\n",
    "\n",
    "# price feature weights\n",
    "weights = np.ravel(theta_price)\n",
    "all_features = price_numeric_features + one_hot_feature_names\n",
    "\n",
    "arr = np.zeros(len(weights), dtype=[('feature', 'U50'), ('weight', 'f8')])\n",
    "arr['feature'] = all_features\n",
    "arr['weight'] = weights\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b033c308a71f814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:06.023496Z",
     "start_time": "2025-11-21T10:55:06.013497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('price_per_m2',  0.59429279), ('size',  0.11431869),\n",
       "       ('district_crime',  0.04548097), ('rooms_',  0.04484099),\n",
       "       ('agent_id_Gregory Dixon',  0.04360975),\n",
       "       ('color_gray',  0.03592666),\n",
       "       ('agent_id_Alice Vadasy',  0.03379704),\n",
       "       ('district_transport',  0.02932396), ('color_white',  0.02794031),\n",
       "       ('advertisement_premium',  0.0263739 ),\n",
       "       ('agent_id_Pat Daniels',  0.02344812),\n",
       "       ('agent_id_John Rios',  0.01829175),\n",
       "       ('agent_id_Kristen Webb',  0.01770776),\n",
       "       ('rooms_3 rooms',  0.01754181), ('rooms_5 rooms',  0.01745443),\n",
       "       ('agent_id_Shirly Delrio',  0.01518605),\n",
       "       ('advertisement_no',  0.01501774),\n",
       "       ('agent_id_Cheryl Dunlap',  0.01398681),\n",
       "       ('agent_id_Mary Chavez',  0.01121092),\n",
       "       ('color_unknown',  0.01087315),\n",
       "       ('agent_id_Gonzalo Ramos',  0.0073066 ),\n",
       "       ('agent_id_Rhonda Bowers',  0.00724697),\n",
       "       ('agent_id_Porfirio Wueste',  0.00680958),\n",
       "       ('agent_id_Pedro Sasser',  0.0055412 ),\n",
       "       ('agent_id_Frank Schmidt', -0.00201539),\n",
       "       ('agent_id_Johnnie Stanley', -0.00254374),\n",
       "       ('agent_id_Lucy Moffatt', -0.00469234),\n",
       "       ('rooms_2 rooms', -0.00611035), ('color_blue', -0.00625401),\n",
       "       ('agent_id_Michael Rowland', -0.00669899),\n",
       "       ('rooms_4 rooms', -0.0078789 ),\n",
       "       ('agent_id_Donald Campbell', -0.00869846),\n",
       "       ('kitchens', -0.01011944), ('agent_id_Ann Perez', -0.01227299),\n",
       "       ('agent_id_Michael Hanners', -0.01346395),\n",
       "       ('color_red', -0.01406992),\n",
       "       ('agent_id_Misty Wallace', -0.01502437),\n",
       "       ('agent_id_Donna Mcclintock', -0.01833016),\n",
       "       ('school_capacity', -0.01851241), ('color_black', -0.02156604),\n",
       "       ('color_green', -0.02452695),\n",
       "       ('agent_id_Charlotte Nodeine', -0.03521884),\n",
       "       ('agent_id_Frank Scheetz', -0.03632564),\n",
       "       ('advertisement_regular', -0.0414983 ),\n",
       "       ('sun_factor', -0.04219114),\n",
       "       ('agent_id_John Jiminez', -0.0454734 ),\n",
       "       ('rooms_1 rooms', -0.10352285),\n",
       "       ('external_storage_m2', -0.11816486),\n",
       "       ('condition_rating', -0.11903658), ('storage_rating', -0.25410903),\n",
       "       ('lot_w', -0.52789008)],\n",
       "      dtype=[('feature', '<U50'), ('weight', '<f8')])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marked feature weights\n",
    "marked_categories_per_feature = [\n",
    "    ['no', 'regular', 'premium'],  # advertisement\n",
    "\n",
    "    ['black', 'blue', 'gray', 'green', 'red', 'unknown', 'white'],  # color\n",
    "\n",
    "    ['', '1 rooms', '2 rooms', '3 rooms', '4 rooms', '5 rooms'],  # rooms\n",
    "\n",
    "    ['Alice Vadasy', 'Rhonda Bowers', 'Pedro Sasser', 'Porfirio Wueste', 'Donna Mcclintock', 'Frank Schmidt', 'John Rios', 'Michael Hanners', 'John Jiminez', 'Frank Scheetz', 'Donald Campbell', 'Kristen Webb', 'Michael Rowland', 'Charlotte Nodeine', 'Ann Perez', 'Shirly Delrio', 'Lucy Moffatt', 'Gonzalo Ramos', 'Cheryl Dunlap', 'Mary Chavez', 'Pat Daniels', 'Johnnie Stanley', 'Misty Wallace', 'Gregory Dixon'] # agent names\n",
    "]\n",
    "\n",
    "# Number of classes per categorical feature\n",
    "marked_n_classes_per_feature = [len(c) for c in marked_categories_per_feature]\n",
    "\n",
    "# Generate one-hot feature names correctly\n",
    "marked_one_hot_feature_names = []\n",
    "for feature_name, categories in zip(marked_categorical_features, marked_categories_per_feature):\n",
    "    for cat in categories:\n",
    "        marked_one_hot_feature_names.append(f\"{feature_name}_{cat}\")\n",
    "\n",
    "# Combine numeric + one-hot feature names\n",
    "all_features = marked_numeric_features + marked_one_hot_feature_names\n",
    "\n",
    "# Flatten weights\n",
    "weights = np.ravel(theta_marked)\n",
    "\n",
    "# Safety check\n",
    "assert len(all_features) == len(weights), f\"Length mismatch: features={len(all_features)}, weights={len(weights)}\"\n",
    "\n",
    "# Create structured array\n",
    "arr = np.zeros(len(weights), dtype=[('feature', 'U50'), ('weight', 'f8')])\n",
    "arr['feature'] = all_features\n",
    "arr['weight'] = weights\n",
    "\n",
    "# Sort descending\n",
    "arr_sorted = np.sort(arr, order='weight')[::-1]\n",
    "\n",
    "arr_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb7de51c23e49d",
   "metadata": {},
   "source": [
    "# Pickle model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae133b150486749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:06.178893Z",
     "start_time": "2025-11-21T10:55:06.173698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save weights using pickle\n",
    "import pickle\n",
    "with open(\"./models/price_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(theta_price, f)\n",
    "\n",
    "with open(\"./models/price_model_bias.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bias_price, f)\n",
    "\n",
    "# with open(\"./models/marked_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(theta_marked, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e47eefc141859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T10:55:06.260648Z",
     "start_time": "2025-11-21T10:55:06.250477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: [2554259.81776624], y: [2687103.] , error [1.05200848]\n",
      "y_hat: [5232496.54722276], y: [4824632.] , error [0.92205164]\n",
      "y_hat: [6368907.00085829], y: [6992047.] , error [1.09784096]\n",
      "y_hat: [11033697.43697369], y: [13268780.] , error [1.20256877]\n",
      "y_hat: [5693032.10024077], y: [5351483.] , error [0.94000577]\n",
      "y_hat: [5705989.00769182], y: [5699283.] , error [0.99882474]\n",
      "y_hat: [12202257.72720428], y: [12744741.] , error [1.04445761]\n",
      "y_hat: [8125956.15858636], y: [8636067.] , error [1.06277549]\n",
      "y_hat: [2165215.03377162], y: [1947098.] , error [0.89926311]\n",
      "y_hat: [12584415.18221549], y: [12605686.] , error [1.00169025]\n",
      "MSE: 0.06367283570915204\n",
      "MAE: 0.18604484977446092\n",
      "R²: 0.9357306746458752\n"
     ]
    }
   ],
   "source": [
    "# predict known data\n",
    "y_pred_log = predict(theta_price, X_train_price_scaled, bias_price)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = y_price[:1880]\n",
    "y_true_scaled = y_train_price_scaled[:1880]\n",
    "\n",
    "e = np.array([y_true / y_pred])\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"y_hat: {y_pred[i]}, y: {y_true[i]} , error {y_true[i] / y_pred[i]}\")\n",
    "\n",
    "# calculate MSE\n",
    "mse = ((y_pred_log - y_true_scaled) ** 2).mean()\n",
    "print(\"MSE:\", mse)\n",
    "# calculate MAE\n",
    "mae = np.abs(y_pred_log - y_true_scaled).mean()\n",
    "print(\"MAE:\", mae)\n",
    "# R^2\n",
    "u = ((y_true_scaled - y_pred_log) ** 2).sum()\n",
    "v = ((y_true_scaled - y_true_scaled.mean()) ** 2).sum()\n",
    "r2 = 1 - (u / v)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4ec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "y_true",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_pred",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pct_error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "abs_pct_error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y_true_over_y_pred",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5e63d426-1c23-4b43-bf80-f430fdc160e4",
       "rows": [
        [
         "0",
         "2687103.0",
         "2554259.817766244",
         "132843.18223375594",
         "4.943732422380383",
         "4.943732422380383",
         "1.052008484536209"
        ],
        [
         "1",
         "4824632.0",
         "5232496.547222764",
         "-407864.54722276423",
         "-8.453796003980495",
         "8.453796003980495",
         "0.9220516356691635"
        ],
        [
         "2",
         "6992047.0",
         "6368907.000858288",
         "623139.9991417117",
         "8.912125435394124",
         "8.912125435394124",
         "1.0978409637725492"
        ],
        [
         "3",
         "13268780.0",
         "11033697.436973693",
         "2235082.563026307",
         "16.844672705601475",
         "16.844672705601475",
         "1.20256877404818"
        ],
        [
         "4",
         "5351483.0",
         "5693032.1002407735",
         "-341549.1002407735",
         "-6.382326174646795",
         "6.382326174646795",
         "0.940005765956189"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "      <th>pct_error</th>\n",
       "      <th>abs_pct_error</th>\n",
       "      <th>y_true_over_y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2687103.0</td>\n",
       "      <td>2.554260e+06</td>\n",
       "      <td>1.328432e+05</td>\n",
       "      <td>4.943732</td>\n",
       "      <td>4.943732</td>\n",
       "      <td>1.052008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4824632.0</td>\n",
       "      <td>5.232497e+06</td>\n",
       "      <td>-4.078645e+05</td>\n",
       "      <td>-8.453796</td>\n",
       "      <td>8.453796</td>\n",
       "      <td>0.922052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6992047.0</td>\n",
       "      <td>6.368907e+06</td>\n",
       "      <td>6.231400e+05</td>\n",
       "      <td>8.912125</td>\n",
       "      <td>8.912125</td>\n",
       "      <td>1.097841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13268780.0</td>\n",
       "      <td>1.103370e+07</td>\n",
       "      <td>2.235083e+06</td>\n",
       "      <td>16.844673</td>\n",
       "      <td>16.844673</td>\n",
       "      <td>1.202569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5351483.0</td>\n",
       "      <td>5.693032e+06</td>\n",
       "      <td>-3.415491e+05</td>\n",
       "      <td>-6.382326</td>\n",
       "      <td>6.382326</td>\n",
       "      <td>0.940006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_true        y_pred         error  pct_error  abs_pct_error  \\\n",
       "0   2687103.0  2.554260e+06  1.328432e+05   4.943732       4.943732   \n",
       "1   4824632.0  5.232497e+06 -4.078645e+05  -8.453796       8.453796   \n",
       "2   6992047.0  6.368907e+06  6.231400e+05   8.912125       8.912125   \n",
       "3  13268780.0  1.103370e+07  2.235083e+06  16.844673      16.844673   \n",
       "4   5351483.0  5.693032e+06 -3.415491e+05  -6.382326       6.382326   \n",
       "\n",
       "   y_true_over_y_pred  \n",
       "0            1.052008  \n",
       "1            0.922052  \n",
       "2            1.097841  \n",
       "3            1.202569  \n",
       "4            0.940006  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pick y_true (prefer y_true, fallback to ys) and y_pred from available variables\n",
    "if 'y_true' in globals():\n",
    "    y_true_arr = np.ravel(y_true)\n",
    "elif 'ys' in globals():\n",
    "    y_true_arr = np.ravel(ys)\n",
    "else:\n",
    "    raise NameError(\"y_true or ys not found in the notebook namespace.\")\n",
    "\n",
    "# pick a suitable prediction variable\n",
    "if 'y_pred' in globals():\n",
    "    y_pred_arr = np.ravel(y_pred)\n",
    "elif 'y_pred_days' in globals():\n",
    "    y_pred_arr = np.ravel(y_pred_days)\n",
    "elif 'y_pred_log' in globals():\n",
    "    # try inverse-transform if predictions are log-scaled\n",
    "    try:\n",
    "        y_pred_arr = np.ravel(np.expm1(y_pred_log))\n",
    "    except Exception:\n",
    "        y_pred_arr = np.ravel(y_pred_log)\n",
    "else:\n",
    "    raise NameError(\"y_pred (or y_pred_days / y_pred_log) not found in the notebook namespace.\")\n",
    "\n",
    "# align lengths\n",
    "n = min(len(y_true_arr), len(y_pred_arr))\n",
    "y_true_arr = y_true_arr[:n]\n",
    "y_pred_arr = y_pred_arr[:n]\n",
    "\n",
    "# compute errors safely (avoid div by zero)\n",
    "eps = 1e-8\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    error = y_true_arr - y_pred_arr\n",
    "    pct_error = error / np.where(np.abs(y_true_arr) > eps, y_true_arr, np.nan) * 100.0\n",
    "    abs_pct_error = np.abs(error) / np.where(np.abs(y_true_arr) > eps, np.abs(y_true_arr), np.nan) * 100.0\n",
    "    ratio = np.where(np.abs(y_pred_arr) > eps, y_true_arr / y_pred_arr, np.nan)\n",
    "\n",
    "df_errors = pd.DataFrame({\n",
    "    'y_true': y_true_arr,\n",
    "    'y_pred': y_pred_arr,\n",
    "    'error': error,\n",
    "    'pct_error': pct_error,            # (y_true - y_pred) / y_true * 100\n",
    "    'abs_pct_error': abs_pct_error,    # absolute percent error\n",
    "    'y_true_over_y_pred': ratio        # y_true / y_pred\n",
    "})\n",
    "\n",
    "df_errors.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
